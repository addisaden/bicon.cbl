In [1]: from tokenizers import Tokenizer

In [2]: tokenizer = Tokenizer.from_pretrained("bert-base-uncased")

In [3]: res = tokenizer.encode("Hallo, mein Name ist Adrian")

In [4]: res
Out[4]: Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])

In [5]: len(res)
Out[5]: 9

In [6]: res.tokens
Out[6]: ['[CLS]', 'hall', '##o', ',', 'mein', 'name', 'ist', 'adrian', '[SEP]']

In [7]: res.type_ids
Out[7]: [0, 0, 0, 0, 0, 0, 0, 0, 0]

In [8]: res.ids
Out[8]: [101, 2534, 2080, 1010, 24182, 2171, 21541, 7918, 102]

In [9]: res.offsets
Out[9]: [(0, 0), (0, 4), (4, 5), (5, 6), (7, 11), (12, 16), (17, 20), (21,27), (0, 0)]

In [10]: res.attention_mask
Out[10]: [1, 1, 1, 1, 1, 1, 1, 1, 1]

In [11]: res.special_tokens_mask
Out[11]: [1, 0, 0, 0, 0, 0, 0, 0, 1]

In [12]: res.overflowing
Out[12]: []
